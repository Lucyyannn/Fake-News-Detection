{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "# 模型存储路径\n",
    "model_dir = Path(\"./bert_test_checkpoints\")\n",
    "# 如果模型目录不存在，则创建一个\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载Bert分词器和预训练模型\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./ernie\")\n",
    "bert_model = BertModel.from_pretrained(\"./ernie\")\n",
    "#加载数据集\n",
    "train_data_first = pd.read_csv(\"./data/train.news.csv\")\n",
    "test_data_first = pd.read_csv(\"./data/test.feature.csv\")\n",
    "\n",
    "#提取'Title'和'label'两列\n",
    "train_data_second = train_data_first.loc[:, ['Title','label']]\n",
    "\n",
    "test_data_second = test_data_first.loc[:, ['Title']]\n",
    "test_data_second['id']=test_data_second.index+1\n",
    "\n",
    "#填补缺失值\n",
    "train_data_second['Title'] =train_data_second['Title'].fillna('')\n",
    "test_data_second['Title'] =test_data_second['Title'].fillna('')\n",
    "\n",
    "#洗牌并划分验证集\n",
    "#按7:1比例划分训练集ds_train和验证集ds_valid（9263，1324）ds_test(10141)\n",
    "valid_data = train_data_second.sample(frac=0.125)\n",
    "train_data = train_data_second[~train_data_second.index.isin(valid_data.index)]\n",
    "test_data=test_data_second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Dataset\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(MyDataset, self).__init__()#调用父类的init，确保父类正确初始化\n",
    "        self.mode = mode\n",
    "        # 拿到对应的数据\n",
    "        if mode == 'train':\n",
    "            self.dataset = train_data\n",
    "        elif mode == 'valid':\n",
    "            self.dataset = valid_data\n",
    "            \n",
    "        elif mode == 'test':\n",
    "            # 如果是测试模式，则返回内容和id。\n",
    "            self.dataset = test_data\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Unknown mode {}\".format(mode))\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # 取第index条\n",
    "        data = self.dataset.iloc[idx]\n",
    "        # 取其内容\n",
    "        text = data['content']\n",
    "        # 根据状态返回内容\n",
    "        if self.mode == 'test':\n",
    "            # 如果是test，将id做为target\n",
    "            label = data['id']\n",
    "        else:\n",
    "            label = data['label']\n",
    "        # 返回内容和label\n",
    "        return text, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_dataset = MyDataset('train')\n",
    "valid_dataset = MyDataset('valid')\n",
    "test_dataset=MyDataset('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[    1,  1060,   464,  ...,     0,     0,     0],\n",
      "        [    1, 17963,   305,  ...,     0,     0,     0],\n",
      "        [    1,   248,    82,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1,   342,   337,  ...,     0,     0,     0],\n",
      "        [    1,  9474,   132,  ...,     0,     0,     0],\n",
      "        [    1,   978,   828,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "targets: tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#构造Dataloader。\n",
    "#自定义collate_fn，将多个样本合成一个批次，在其中完成对句子进行编码、填充、组装batch：\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    text, label = zip(*batch)\n",
    "    text, label = list(text), list(label)\n",
    "\n",
    "    # src给bert\n",
    "    # padding='max_length' 不够长度的进行填充\n",
    "    # truncation=True 长度过长的进行裁剪\n",
    "    src = tokenizer(text, padding='max_length', max_length=180, return_tensors='pt', truncation=True)\n",
    "    \n",
    "    return src, torch.LongTensor(label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"inputs:\", inputs)\n",
    "print(\"targets:\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#定义预测模型，由bert模型加上最后的预测层组成\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # 加载bert预训练模型bert_model\n",
    "        self.bert = bert_model\n",
    "        # 最后的预测层\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src直接序列解包传入bert\n",
    "        # 得到encoder的输出，用最前面[CLS]的输出作为最终线性层的输入\n",
    "        outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
    "    \n",
    "        return self.predictor(outputs)\n",
    "  \n",
    "model = MyModel()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "#损失函数Binary Cross Entropy：\n",
    "criteria = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "y_true = []  # 验证集实际标签\n",
    "y_pred = []  # 验证集模型预测结果\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_correct = 0\n",
    "    for inputs, targets in valid_loader:\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        correct_num = (((outputs >= 0.5).float() * 1).flatten() == targets).sum()\n",
    "        total_correct += correct_num\n",
    "        #保留y_true, y_pred, 计算precision,Recall,F1-score,AUC\n",
    "        predictions = (outputs >= 0.5).float()  \n",
    "        y_true.extend(targets.tolist())\n",
    "        y_pred.extend(predictions.view(-1).tolist())\n",
    "\n",
    "    return total_correct / len(valid_dataset), total_loss / len(valid_dataset), y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 首先将模型调成训练模式\\nmodel.train()\\n#设置loss辅助变量\\ntotal_loss = 0.\\nstep = 0# 记录步数\\nlog_per_step = 50#每个50步打印一次loss\\nbest_accuracy = 0# 记录在验证集上最好的准确率\\n\\nepochs=4\\n# 开始训练\\nfor epoch in range(epochs):\\n    model.train()\\n    for i, (inputs, targets) in enumerate(train_loader):\\n        # 从batch中拿到训练数据\\n        inputs, targets = to_device(inputs), targets.to(device)\\n        # 传入模型进行前向传播\\n        outputs = model(inputs)\\n        # 计算损失\\n        loss = criteria(outputs.view(-1), targets.float())\\n        #反向传播\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n\\n        total_loss += float(loss)\\n        step += 1\\n\\n        if step % log_per_step == 0:\\n            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\\n            total_loss = 0\\n\\n        del inputs, targets\\n\\n    # 一个epoch后，使用验证集进行验证\\n    accuracy, validation_loss, y_trues, y_preds = validate()\\n    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))  \\n    Precision=precision_score(y_true,y_pred)\\n    Recall=recall_score(y_true,y_pred)\\n    F1_score = f1_score(y_true, y_pred)\\n    auc = roc_auc_score(y_true, y_pred) \\n    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\\n    # 保存最好的模型\\n    if accuracy > best_accuracy:\\n        torch.save(model, model_dir / f\"model_test_best.pt\")\\n        best_accuracy = accuracy\\n        best_pre=Precision\\n        best_re=Recall\\n        best_F1=F1_score\\n        best_auc=auc\\n\\n        curve_pre=precision\\n        curve_re=recall\\n    \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 首先将模型调成训练模式\n",
    "model.train()\n",
    "#设置loss辅助变量\n",
    "total_loss = 0.\n",
    "step = 0# 记录步数\n",
    "log_per_step = 50#每个50步打印一次loss\n",
    "best_accuracy = 0# 记录在验证集上最好的准确率\n",
    "\n",
    "epochs=4\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # 从batch中拿到训练数据\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        # 传入模型进行前向传播\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        #反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        step += 1\n",
    "\n",
    "        if step % log_per_step == 0:\n",
    "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "        del inputs, targets\n",
    "\n",
    "    # 一个epoch后，使用验证集进行验证\n",
    "    accuracy, validation_loss, y_trues, y_preds = validate()\n",
    "    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))  \n",
    "    \n",
    "    # 保存最好的模型\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model, model_dir / f\"model_test_best.pt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_dir / f\"model_test_best.pt\",map_location='cpu')\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "for inputs, ids in test_loader:\n",
    "    outputs = model(inputs.to(device))\n",
    "    outputs = (outputs >= 0.3).int().flatten().tolist()\n",
    "    ids = ids.tolist()\n",
    "    results = results + [(id, result) for result, id in zip(outputs, ids)]\n",
    "\n",
    "test_label = [pair[1] for pair in results]\n",
    "test_data['label'] = test_label\n",
    "test_data[['id','label']].to_csv('submit_task2.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

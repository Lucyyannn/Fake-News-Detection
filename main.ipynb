{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\25842\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#导入库和数据\n",
    "import pyprind \n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.layers import Embedding \n",
    "from keras import Sequential\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import datasets as tfds\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import AUC\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#数据读取\n",
    "data_first=pd.read_csv(\"./data/train.news.csv\")\n",
    "TestData_first=pd.read_csv(\"./data/test.feature.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\25842\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.038 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Title  label\n",
      "0      [15, 5189, 6442, 5190, 65, 4134, 49, 10, 1311,...      0\n",
      "1      [3513, 69, 70, 1405, 71, 432, 4, 2126, 45, 644...      0\n",
      "2      [278, 202, 101, 560, 168, 12, 201, 286, 1777, ...      0\n",
      "3      [4136, 193, 279, 834, 292, 506, 176, 791, 9, 6...      0\n",
      "4      [133, 36, 2127, 507, 124, 77, 4, 20, 5, 154, 2...      0\n",
      "...                                                  ...    ...\n",
      "10582  [47, 12, 43, 92, 67, 24, 19, 18, 54, 52, 35, 0...      1\n",
      "10583  [454, 147, 44, 36, 24, 20, 11119, 1, 0, 0, 0, ...      1\n",
      "10584  [239, 2624, 610, 2, 11120, 9, 57, 318, 5061, 4...      1\n",
      "10585  [653, 557, 18, 124, 83, 77, 4, 36, 858, 600, 3...      1\n",
      "10586  [298, 4015, 445, 4930, 4016, 1291, 4017, 2420,...      1\n",
      "\n",
      "[10587 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#数据处理\n",
    "stem = PorterStemmer()\n",
    "punc = r'~`!#$%^&*()_+-=|\\';\":/.,?><~·！@#￥%……&*（）——+-=“：’；、。，？》《{}'\n",
    "\n",
    "\n",
    "def stopwordslist(filepath):\n",
    "    stop_words = [line.strip() for line in open( filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stop_words\n",
    "\n",
    "stopwords = stopwordslist('chinesestopwords.txt')\n",
    "\n",
    "def cleaning(text):\n",
    "    cutwords = list(jieba.lcut_for_search(text))\n",
    "    final_cutwords = ''\n",
    "    for word in cutwords:\n",
    "        if word not in stopwords and punc:\n",
    "            final_cutwords += word+\" \"\n",
    "    return final_cutwords\n",
    "'''\n",
    "'''\n",
    "t = pd.DataFrame(data_second.astype(str))\n",
    "data_second[\"Title\"] =  t[\"Ofiicial Account Name\"] + t[\"Title\"] + t[\"Report Content\"]\n",
    "tt = pd.DataFrame(TestData_second.astype(str))\n",
    "TestData_second[\"Title\"] = tt[\"Ofiicial Account Name\"] + tt[\"Title\"] + tt[\"Report Content\"]\n",
    "#数据清洗\n",
    "data_second[\"Title\"] = data_second[\"Title\"].apply(cleaning)\n",
    "TestData_second[\"Title\"] = TestData_second[\"Title\"].apply(cleaning)\n",
    "'''\n",
    "#数据处理\n",
    "#提取两列：title和label，得到data_second\n",
    "data_second=data_first.loc[:,['Title','label']]#10587\n",
    "TestData_second=TestData_first.loc[:,['Title']]#10141\n",
    "#转换为字符串\n",
    "#对data_second进行分词\n",
    "data_second['Title'] = data_second['Title'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "TestData_second['Title'] = TestData_second['Title'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "#对data_second中的文本进行编码映射，并将文本转换成编码\n",
    "#构建词表\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n，。',)\n",
    "tokenizer.fit_on_texts(data_second['Title'])\n",
    "#word_size=16302\n",
    "word_size=len(tokenizer.index_word)\n",
    "# 将词转换为整数序列\n",
    "tokens = tokenizer.texts_to_sequences(data_second['Title'])\n",
    "Test_tokens=tokenizer.texts_to_sequences(TestData_second['Title'])\n",
    "# 对整数序列进行补齐\n",
    "maxlen = max(len(sequence) for sequence in tokens)#39\n",
    "tokens_pad = sequence.pad_sequences(tokens, maxlen=58, padding='post', truncating='pre')\n",
    "\n",
    "Test_maxlen=max(len(sequence) for sequence in Test_tokens)#58\n",
    "Test_tokens_pad = sequence.pad_sequences(Test_tokens, maxlen=58, padding='post', truncating='pre')\n",
    "# 构建新的DataFrame\n",
    "data_third = pd.DataFrame({'Title': tokens_pad.tolist(), 'label': data_second['label']})\n",
    "TestData_third=pd.DataFrame({'Title': Test_tokens_pad.tolist()})\n",
    "print(data_third)\n",
    "#创建tensorflow\n",
    "target=data_third.pop('label')\n",
    "target_tensor = tf.convert_to_tensor(target)\n",
    "# 将每个numpy数组转换为tensor对象tensor_list\n",
    "tensor_list = data_third['Title'].apply(tf.convert_to_tensor)\n",
    "Test_tensor_list=TestData_third['Title'].apply(tf.convert_to_tensor)\n",
    "# 将tensor对象堆叠成一个新的tensor对象ds_raw\n",
    "ds_raw=tf.stack(tensor_list)\n",
    "Test_ds_raw=tf.stack(Test_tensor_list)\n",
    "#创建联合数据集dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((ds_raw, target_tensor))\n",
    "Test_dataset=tf.data.Dataset.from_tensor_slices(Test_ds_raw)\n",
    "#洗牌\n",
    "tf.random.set_seed(1)\n",
    "dataset=dataset.shuffle(10587,reshuffle_each_iteration=False)\n",
    "#按7：1比例划分训练集ds_train和验证集ds_valid（9263，1324）\n",
    "ds_train=dataset.take(9263)\n",
    "ds_valid=dataset.skip(9263)\n",
    "ds_test=Test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\25842\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, None, 58)          945632    \n",
      "                                                                 \n",
      " bi-lstm_1 (Bidirectional)   (None, None, 128)         62976     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " bi-lstm_2 (Bidirectional)   (None, 128)               98816     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1115745 (4.26 MB)\n",
      "Trainable params: 1115745 (4.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#建立Bi-LSTM模型\n",
    "embedding_dim=58\n",
    "vocab_size=word_size+2  #n->n+2\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# 定义L2正则化项\n",
    "l2_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "#模型\n",
    "bi_lstm_model=tf.keras.Sequential([tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,name='embed-layer'),\n",
    "                                   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,\n",
    "                                                                                      name='lstm-layer',\n",
    "                                                                                      return_sequences=True,\n",
    "                                                                                      kernel_regularizer=l2_regularizer, \n",
    "                                                                                      recurrent_regularizer=l2_regularizer),\n",
    "                                                                                      name='bi-lstm_1'),\n",
    "                                   tf.keras.layers.Dropout(0.1),\n",
    "                                   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, \n",
    "                                                                                      return_sequences=False,\n",
    "                                                                                      kernel_regularizer=l2_regularizer, \n",
    "                                                                                      recurrent_regularizer=l2_regularizer),\n",
    "                                                                                      name='bi-lstm_2'),\n",
    "                                   tf.keras.layers.Dropout(0.1),\n",
    "                                   tf.keras.layers.Dense(64,activation='relu'),\n",
    "                                   tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "bi_lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\25842\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\25842\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "290/290 [==============================] - 39s 81ms/step - loss: 0.4291 - accuracy: 0.8978 - auc: 0.9363 - val_loss: 0.1496 - val_accuracy: 0.9592 - val_auc: 0.9879\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25842\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 21s 74ms/step - loss: 0.0946 - accuracy: 0.9777 - auc: 0.9955 - val_loss: 0.1072 - val_accuracy: 0.9721 - val_auc: 0.9915\n",
      "Epoch 3/10\n",
      "290/290 [==============================] - 18s 63ms/step - loss: 0.0493 - accuracy: 0.9887 - auc: 0.9985 - val_loss: 0.1060 - val_accuracy: 0.9705 - val_auc: 0.9839\n",
      "Epoch 4/10\n",
      "290/290 [==============================] - 16s 54ms/step - loss: 0.0358 - accuracy: 0.9921 - auc: 0.9993 - val_loss: 0.1088 - val_accuracy: 0.9690 - val_auc: 0.9859\n",
      "Epoch 5/10\n",
      "290/290 [==============================] - 17s 57ms/step - loss: 0.0324 - accuracy: 0.9924 - auc: 0.9991 - val_loss: 0.1054 - val_accuracy: 0.9675 - val_auc: 0.9871\n",
      "Epoch 6/10\n",
      "290/290 [==============================] - 18s 61ms/step - loss: 0.0282 - accuracy: 0.9926 - auc: 0.9997 - val_loss: 0.1050 - val_accuracy: 0.9698 - val_auc: 0.9856\n",
      "Epoch 7/10\n",
      "290/290 [==============================] - 32s 110ms/step - loss: 0.0262 - accuracy: 0.9930 - auc: 0.9993 - val_loss: 0.1153 - val_accuracy: 0.9721 - val_auc: 0.9812\n",
      "Epoch 8/10\n",
      "290/290 [==============================] - 32s 109ms/step - loss: 0.0245 - accuracy: 0.9924 - auc: 0.9995 - val_loss: 0.0978 - val_accuracy: 0.9728 - val_auc: 0.9873\n",
      "Epoch 9/10\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 0.0222 - accuracy: 0.9936 - auc: 0.9996 - val_loss: 0.1029 - val_accuracy: 0.9705 - val_auc: 0.9883\n",
      "Epoch 10/10\n",
      "290/290 [==============================] - 32s 111ms/step - loss: 0.0207 - accuracy: 0.9934 - auc: 0.9994 - val_loss: 0.1033 - val_accuracy: 0.9705 - val_auc: 0.9859\n",
      "317/317 [==============================] - 16s 33ms/step\n",
      "[[3.53391422e-03]\n",
      " [1.24508776e-01]\n",
      " [4.11046552e-04]\n",
      " ...\n",
      " [1.13159954e-03]\n",
      " [9.05207344e-05]\n",
      " [9.76052888e-06]]\n",
      "10141\n"
     ]
    }
   ],
   "source": [
    "#分批\n",
    "TrainData=ds_train.padded_batch(32,padded_shapes=([-1],[]))\n",
    "ValidData=ds_valid.padded_batch(32,padded_shapes=([-1],[]))\n",
    "\n",
    "#配置模型\n",
    "bi_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy',tf.keras.metrics.AUC()])\n",
    "'''\n",
    "# 创建EarlyStopping回调函数\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "'''\n",
    "checkpoint = ModelCheckpoint('checkpoint.hdf5', monitor='val_loss', save_best_only=True,mode='auto')\n",
    "history=bi_lstm_model.fit(TrainData,validation_data=(ValidData),batch_size=32,epochs=10,callbacks=[checkpoint])\n",
    "#测试，测试数据Test_dataset是tensor联合数据集,分批得到TestData\n",
    "\n",
    "result_label = []\n",
    "TestData=ds_test.batch(32)\n",
    "test_result=bi_lstm_model.predict(TestData)\n",
    "for i in test_result:\n",
    "    if(i>=0.35):\n",
    "        result_label.append(1)\n",
    "    else:\n",
    "        result_label.append(0)\n",
    "print(test_result)#应该是0/1\n",
    "print(len(result_label))\n",
    "\n",
    "id_column = pd.DataFrame({'id': range(1, 10142)})\n",
    "label_column = pd.DataFrame({'label': result_label})\n",
    "submission = pd.concat([id_column,label_column], axis=1)\n",
    "submission.to_csv('submit.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
